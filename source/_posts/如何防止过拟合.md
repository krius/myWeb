---
title: 如何防止过拟合
excerpt: 这是文章摘要呢！
date: 2025-08-12 22:14:24
categories:
tags:
---

过拟合具体的表现就是，模型在训练集上表现很好，但是泛化能力差，也就是说在验证集与测试集的效果不佳。过拟合是机器学习和深度学习早期就会面临的障碍，我们要认识到过拟合的本质以及如何“缓解”，下面将介绍几种方法。
## 数据增强（Data Augmentation）
通过增加训练集的大小，增加数据的多样性，让模型学习到任务的更多特征，避免记住训练集的特定分布（也就是说我们收集到的数据集，之中的数据分布，不一定与真实分布相同），导致的过拟合现象。对于图像数据，常见的有平移、旋转之类的操作，同时改变颜色图像中添加白噪声等都是常规的操作，需要根据当前的任务特点以及模型特点来考虑。比如图像数据的增强，很多都是基于**卷积神经网络** CNN的**平移不变性**做出来的，**这里的数据增强需要结合模型特点以及任务特点综合考虑。**
## 模型改进
过拟合主要是两个原因造成的，**第一是训练数据太少**，数据增强用来解决这个，**第二是模型可学习的参数太多**，就是模型学习能力过强，从而”记住”了训练集的全部分布，并没有学到真实分布的特点，导致的过拟合。所以，可以通过对模型的简单化或者其他方式抑制模型的表达。
### 早停（Early Stopping）
通过前面的概念，在数据集适中的情况下，过拟合是模型过度学习的过程，因此可以通过模型在训练过程中验证集的表现来监控模型的变化，模型损失的变化曲线通常来说是一个 **“先降后升”** 的过程，通过监听每一轮训练的验证集损失变化情况，来找到**模型损失的最低点**，这一轮我们通常认为是模型最优的情况，在这之后就是过拟合（overfitting），这之前呢也叫做欠拟合（underfitting），表明模型可以进一步训练达到更好的效果。

![](/source/images/过拟合.png)

### 正则化（Regularization）

### Dropout